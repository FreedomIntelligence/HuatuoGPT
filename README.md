# HuatuoGPT (åä½—GPT), Towards Taming Language Models To Be a Doctor.

## âœ¨ Latest News
- [12/11/2023]: ğŸ‰ğŸ‰ğŸ‰ Our paper is accepted for EMNLP 2023! Check it out [here](https://aclanthology.org/2023.findings-emnlp.725/).
- [11/25/2023]: We realeased **[HuatuoGPT-II](https://github.com/FreedomIntelligence/HuatuoGPT-II)**, which achieved a new state-of-the-art in Chinese medical applications! See [here](https://github.com/FreedomIntelligence/HuatuoGPT-II).
- [09/26/2023]: Release [HuatuoGPT-reward-model](https://huggingface.co/FreedomIntelligence/HuatuoGPT-reward-model-7B).
- [06/30/2023]: Evaluation data of HuatuoGPT released in the `eval/` folder.
- [06/30/2023]: Release the code, model weights of [HuatuoGPT-7B](https://huggingface.co/FreedomIntelligence/HuatuoGPT-7B) and [HuatuoGPT-13B](https://huggingface.co/FreedomIntelligence/HuatuoGPT-13b-delta)
- [05/25/2023]: Release the [tech report](https://arxiv.org/pdf/2305.15075.pdf) and the HuatuoGPT [demo](https://www.huatuogpt.cn/).

## âš¡ Introduction
Welcome to the repository of HuatuoGPT, a large language model (LLM) trained on a vast Chinese medical corpus. Our objective with HuatuoGPT is to construct a more professional â€˜ChatGPTâ€™ for medical consultation scenarios. 

Here is a list of what has been released:

1. HuatuoGPT-SFT-data: A hybrid SFT data capitalizing on both strengths to endow the model with Doctor-like and Patient-friendly characteristics.
2. HuatuoGPT model: HuatuoGPT model weights(HuatuoGPT-7B and HuatuoGPT-13B) and the online demo. **HuatuoGPT-7B** is trained on **Baichuan-7B** and **HuatuoGPT-13B** is trained on **Ziya-LLaMA-13B-Pretrain-v1**.
3. Medical evaluation benchmark: an evaluation method used to evaluate LLMs in medical scenarios.

<div align=center>
<img src="assets/huatuo.png" width = "640" alt="HuatuoGPT" align=center/>
</div>


## ğŸ’­ Motivation
- To address the growing demand for quick medical consultations both online and in hospitals that do not necessarily require deep medical knowledge. We believe that LLMs like HuatuoGPT can be effectively utilized to meet these demands, freeing up physiciansâ€™ time and energy for more complex cases.
- To provide open data for training medical LLMs. Building high-quality instruction training data for LLMs is essential, but it can be also challenging. We have constructed medical instruction data using various methods and made it publicly available. This dataset can be combined with other datasets to train one's own medical 'ChatGPT'.
- To emphasize the importance of carefully evaluating the ability of medical LLMs before using them to offer medical assistance to patients. We recognize the potential benefits of LLMs in the medical field, but also acknowledge the need for thorough evaluation and testing to ensure patient safety and accurate diagnoses.

## ğŸ“š Data

### Overview

To leverage the best of both distilled data (from ChatGPT) and real-world data (from Doctors), we firstly fine-tune  `HuatuoGPT`  using four types of data:

| Type                          | Scale | Descriptions                                                 | Examples                                                     |
| ------------------------------------ | ----- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Distilled Instructions from ChatGPT  | 61,400   | Medical instructions generated with self-instruct. Seed instructions are manually written by us. | ğŸ’¬ï¼šç°åœ¨è¯¥åšä»€ä¹ˆå‡†å¤‡æ‰‹æœ¯å‰çš„èº«ä½“æ£€æŸ¥ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šåœ¨å‡†å¤‡æ‰‹æœ¯å‰çš„èº«ä½“æ£€æŸ¥ä¸­ï¼Œéœ€è¦è¿›è¡Œä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š<br>&emsp;&emsp;1. äº†è§£ç—…äººçš„ç—…å²ï¼šäº†è§£ç—…äººçš„ç—…å²ï¼ŒåŒ…æ‹¬æ—¢å¾€ç—…å²ã€æ‰‹æœ¯å²ã€è¯ç‰©è¿‡æ•å²ã€å®¶æ—ç—…å²ç­‰ï¼Œä»¥ä¾¿è¯„ä¼°æ‰‹æœ¯é£é™©å’Œåˆ¶å®šæ‰‹æœ¯æ–¹æ¡ˆã€‚<br>&emsp;&emsp;2. è¿›è¡Œä½“æ ¼æ£€æŸ¥ï¼šè¿›è¡Œå…¨é¢çš„ä½“æ ¼æ£€æŸ¥ï¼ŒåŒ…æ‹¬æµ‹é‡èº«é«˜ã€ä½“é‡ã€è¡€å‹ã€å¿ƒç‡ã€å‘¼å¸â€¦ |
| Real-world Instructions from Doctors | 69,768  | Medical single-round medical consultants in real-world scenarios, in which the doctor's response is more specific and can address the user's medical concerns. | ğŸ’¬ï¼šæˆ‘æœ‰ç³–å°¿ç—…è¡€å‹160-90.è¯¥åƒä»€ä¹ˆé™å‹è¯ï¼Ÿç©ºè…¹è¡€ç³–8å·¦å³ã€‚é¤å11å·¦å³ã€‚æˆ‘æœ‰ç³–å°¿ç—…è¡€å‹160-90.è¯¥åƒä»€ä¹ˆé™å‹è¯ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šé˜…è¯»ä½ çš„æé—®æè¿°ï¼Œç»¼åˆåˆ†æç—‡çŠ¶ï¼Œä½“å¾ï¼Œç³–å°¿ç—…ï¼Œè¡€å‹160-90.è¡€å‹ç•¥é«˜ï¼Œç©ºè…¹è¡€ç³–8å·¦å³ã€‚é¤å11å·¦å³ã€‚è¡€ç³–ç•¥é«˜ã€‚å£æœä¾é‚£æ™®åˆ©ï¼ŒäºŒç”²åŒèƒç‰‡æ²»ç–—ï¼Œæ–°è¿‘ç ”ç©¶ï¼ŒäºŒç”²åŒèƒç‰‡ï¼Œå¯ä»¥æŠ‘åˆ¶é£Ÿæ¬²ï¼Œä¿ƒè¿›è‚ é“è •åŠ¨ï¼Œé¢„é˜²ç™Œç—‡ï¼Œé¢„é˜²å¿ƒè„‘è¡€ç®¡ç—…ï¼Œå»¶é•¿å¯¿å‘½ä½œç”¨ã€‚ |
| Distilled Conversations from ChatGPT     | 68,888   | Medical multi-turn conversations are created using two ChatGPTs: one acting as the patient and the other as the doctor. The conversations are based on real-world patient cases and their corresponding treatment. | ğŸ’¬ï¼šUser: æˆ‘æƒ³é—®ä¸€ä¸‹å¤§å¤«ï¼Œæˆ‘å‘çƒ§ä¸‰å¤©åï¼Œç¬¬å››å¤©å…¨èº«èµ·äº†ç–¹å­ï¼Œæ˜¯æ€ä¹ˆå›äº‹å‘¢ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼š~æ‚¨å¥½ï¼æ„Ÿè°¢æ‚¨çš„æé—®ã€‚æ‚¨çš„ç—‡çŠ¶å¯èƒ½ä¸å¹¼å„¿æ€¥ç–¹ç›¸å…³ã€‚å¹¼å„¿æ€¥ç–¹æ˜¯ä¸€ç§å¸¸è§çš„å„¿ç«¥ç—…ï¼Œç—…å› ä¸ºäººç±»ç–±ç–¹ç—…æ¯’6å‹æˆ–7å‹æ„ŸæŸ“å¼•èµ·ã€‚å®ƒçš„ç—‡çŠ¶åŒ…æ‹¬ï¼š<br> 1. å‘çƒ­ n2. å…¨èº«èµ·çº¢è‰²ç–¹å­ <br> 3. å£è…”å†…åŠå’½å–‰éƒ¨æœ‰å°æ°´æ³¡<br> 4. ç—…ç¨‹ä¸€èˆ¬ä¸º3-5å¤©å·¦å³...<br>ğŸ’¬ï¼šè°¢è°¢å¤§å¤«ï¼é™¤äº†å¹¼å„¿æ€¥ç–¹è¿˜æœ‰å…¶ä»–å¯èƒ½å—ï¼Ÿæˆ‘ä¹‹å‰æœ‰æ„Ÿå†’ï¼Œè¿™å’Œç–¹å­æœ‰å…³ç³»å—ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šé™¤äº†å¹¼å„¿æ€¥ç–¹ï¼Œç—…æ¯’æ€§æ„Ÿå†’å’Œå…¶ä»–ç—…æ¯’æ„ŸæŸ“ä¹Ÿæœ‰å¯èƒ½å¼•èµ·ç–¹å­ï¼Œä½†è¿™ç§æƒ…å†µç›¸å¯¹è¾ƒå°‘è§ã€‚åœ¨æ‚¨çš„æƒ…å†µä¸‹ï¼Œæ„Ÿå†’å¯èƒ½æ˜¯ä¸€ç§è¯±å‘å› ç´ ï¼Œä½†ä¸»è¦è¿˜æ˜¯ç”±ç—…æ¯’æ„ŸæŸ“å¼•èµ·çš„ç–¹å­... |
| Real-world Conversations with Doctors      | 25,986   | Medical multi-round conversations in real-world scenarios. Conversations include multiple rounds of questioning by doctors and continuous questioning by patients. | ğŸ’¬ï¼šé¢å¤´ä¸‹å·´ä¸‹é•¿æ»¡ç—˜ç—˜ã€‚å°±æ˜¯ä¸çŸ¥é“.<br>ğŸ‘¨â€âš•ï¸ï¼šä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå›°æ‰°å—ï¼Ÿæˆ‘å¯ä»¥ä¸ºä½ æä¾›å¸®åŠ©ã€‚è¯·é—®æ‚¨å¹³æ—¶çš„é¥®é£Ÿä¹ æƒ¯ã€ç¡çœ å’ŒæŠ¤è‚¤ä¹ æƒ¯æ€æ ·ï¼ŸåŒ–å¦†å“å’Œé˜²æ™’éœœçš„ä½¿ç”¨é¢‘ç‡å¦‚ä½•ï¼Ÿè¿˜æœ‰æ´é¢çš„é¢‘æ¬¡æ˜¯æ€æ ·çš„å‘¢ï¼Ÿ<br>ğŸ’¬ï¼šå¹³æ—¶æˆ‘åŸºæœ¬ä¸Šåªæ˜¯ç”¨æ°´ä¹³éœœï¼Œç„¶åæ´—é¢å¥¶åªç”¨ä¸€ç§ã€‚æ—¥å¸¸é˜²æ™’åªæœ‰åœ¨å¤å¤©æ—¶ä¼šä½¿ç”¨ï¼Œæˆ‘ä¸€èˆ¬ç”¨æ‚¦è¯—é£åŸçš„æ´—é¢å¥¶ä»¥åŠAHCçš„æŠ¤è‚¤äº§å“ï¼Œè¿™äº›éƒ½æ¯”è¾ƒæ¸©å’Œã€‚<br>ğŸ‘¨â€âš•ï¸ï¼šè°¢è°¢æ‚¨çš„æè¿°ã€‚é¦–å…ˆï¼Œå»ºè®®æ‚¨åœç”¨åŒ–å¦†å“å’Œæ™®é€šæŠ¤è‚¤å“ï¼Œä½¿ç”¨ç‰¹å®šçš„è¯å¦†å“ç‰Œï¼Œå¦‚é›…æ¼¾å’Œç†è‚¤æ³‰çš„æ´é¢æ³¡æ²«æˆ–æ´é¢ä¹³ï¼Œå¹¶é¿å…ä½¿ç”¨çƒ­æ°´æˆ–è‚¥çš‚æ¸…æ´—é¢éƒ¨ã€‚å»ºè®®æ‚¨å¤–ç”¨é˜¿è¾¾å¸•æ—å‡èƒ¶æ¶‚äºé¢éƒ¨ç—¤ç–®ï¼Œä¸€èˆ¬å¤œé—´ä½¿ç”¨ï¼Œæ¶‚æŠ¹åäº”åˆ†é’Ÿä¹‹åå¤–ç”¨åŒ»å­¦æŠ¤è‚¤å“æ¯”å¦‚é›…æ¼¾ã€è–‡èµ„ã€ç†è‚¤æ³‰æ¸…çˆ½å‹çš„èˆ’ç¼“é¢éœœï¼Œæˆ–è€…æ˜¯ç»´ç”Ÿç´ eä¹³è†â€¦ |

### Download
- [HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1): The data used in the Supervised Fine-Tuning (SFT) stage of HuatuoGPT.

  

## ğŸ‘¨â€âš•ï¸ Model

### Model Access
| Model                | Backbone      | Link                                                                          |
|----------------------|---------------|-------------------------------------------------------------------------------|
| HuatuoGPT-13B | Ziya-LLaMA-13B-Pretrain-v1 | [Delta](https://huggingface.co/FreedomIntelligence/HuatuoGPT-13b-delta) |
| HuatuoGPT-7B      | Baichuan-7B | [Model Weights](https://huggingface.co/FreedomIntelligence/HuatuoGPT-7B)      |


Note that due to that HuatuoGPT-13B-delta is a LLaMA based model, we only release the delta of weights. You can download LLaMA-13B weights and use apply_delta.py to convert:
```bash 
python apply_delta.py \
--base-model-path $LLaMA_Base_Path \
--target-model-path $Save_Path \
--delta-path $Delta_Path
```

### Deploy

Firstly, you should install all required packages
```bash
pip install -r requirements.txt
```

Please make sure you have download our model weights and run
```bash
python huatuo_cli_demo_stream.py --model-name $model_dir
```



## ğŸš€ Demo

Try our model in [https://www.huatuogpt.cn/](https://www.huatuogpt.cn/). Note that it is still in progressing.

<!-- ![demo_1](assets/demo_1.png) -->
<!-- ![demo_2](assets/demo_2.png) -->



## ğŸ§ Evaluations

### Evaluation by GPT-4 and Doctors
We invite GPT-4 and doctors to compare responses from HuatuoGPT(13B version) and other LLMs. Evaluation data is available in the `eval/` folder. Results are as below:

- Single turn evaluation

<div align=center>
<img src="assets/single_turn_compare.png"  alt="eval1" align=center/>
</div>

- Multi turn evaluation
<div align=center>
<img src="assets/multi_turn_compare.png"  alt="eval1" align=center/>
</div>


### Benchmark  Evaluation

| Dataset   | Model | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | GLEU | ROUGE-1 | ROUGE-2 | ROUGE | Distinct-1 | Distinct-2 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| cMedQA2 | T5-finetuned | 20.88 | 11.87 | 7.69 | 5.09 | 7.62 | 27.16 | 9.30 | 20.11 | 0.41 | 0.52 |
|  | HuatuoGPT | 27.39 | 14.38 | 8.06 | 4.55 | 8.52 | 29.26 | 8.02 | 15.46 | 0.74 | 0.93 |
| WebMedQA | T5-finetuned | 21.42 | 13.79 | 10.06 | 7.38 | 8.94 | 31.00 | 13.85 | 25.78 | 0.37 | 0.46 |
|  | HuatuoGPT | 24.85 | 13.42 | 7.72 | 4.51 | 7.50 | 28.30 | 7.72 | 14.50 | 0.73 | 0.93 |
| Huatuo-26M | T5-finetuned | 26.63 | 16.74 | 11.77 | 8.46 | 11.38 | 33.21 | 13.26 | 24.85 | 0.51 | 0.68 |
|  | HuatuoGPT | 27.42 | 14.84 | 8.54 | 4.96 | 8.01 | 29.16 | 8.29 | 15.84 | 0.74 | 0.93 |

## âš’ï¸ Training
### Prepare the Data
You can download the SFT data from [HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1) or buld your SFT data as the same schema.

### Training
You can train the model by:
```bash
accelerate launch \
	--config_file scripts/sft.yaml \
	--num_processes 8 \
	--num_machines 1 \
	--machine_rank 0 \
	--deepspeed_multinode_launcher standard scripts/finetune.py \
    --experiment_name HuatuoGPT \
	--model_path /path/to/your/model \
    --gradient_accumulation_steps 8 \
    --max_ckpts 3 \
    --max_seq_len 2048 \
	--data_dir /path/to/your/data \
	--output_dir ./ckpts \
	--log_dir ./train_logs \
	--n_epochs 3 \
	--train_bsz_per_gpu 2 \
	--eval_bsz_per_gpu 2 \
	--learning_rate 5e-5 \
	--eval_step -1 \
	--save_step -1 \
    --gradient_checkpointing
```

## ğŸ¤– Limitations

Our goal with HuatuoGPT is to address the need for quick medical consultations, rather than replace doctors or provide full medical support to patients. However, our model does have several limitations that must be taken into consideration:

- Misunderstandings: As with all language models, there is a risk of misunderstandings or misinterpretations, especially when dealing with medical jargon or complex conditions. In this scenario, our models may give wrong answers.
- Hallucinations: Large language models can sometimes generate responses that do not make sense or are completely unrelated to the given input. These "hallucinations" can be especially problematic when users are not familiar with the concepts being discussed, as they may not be able to easily recognize the errors in the model's output. These "hallucinations" can be a challenge to detect and avoid.
- Bias: LLMs are trained on large datasets, which can inadvertently introduce bias into the model's responses. Additionally, care should be taken to ensure that the model is not used to perpetuate biases in medical treatment.

## Acknowledgement

We are aware that our works are inspired by the following works, including but not limited to

- IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1: https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1
- Baichuan-7B: https://huggingface.co/baichuan-inc/baichuan-7B
- LLaMA: https://arxiv.org/abs/2302.13971
- Self-instruct: https://github.com/yizhongw/self-instruct

Without these, nothing could happen in this repository.

## Citation
```angular2
@article{huatuogpt-2023,
  title={HuatuoGPT, Towards Taming Language Models To Be a Doctor},
  author={Hongbo Zhang and Junying Chen and Feng Jiang and Fei Yu and Zhihong Chen and Jianquan Li and Guiming Chen and Xiangbo Wu and Zhiyi Zhang and Qingying Xiao and Xiang Wan and Benyou Wang and Haizhou Li},
  journal={arXiv preprint arXiv:2305.15075},
  year={2023}
}
```

We are from the School of Data Science, the Chinese University of Hong Kong, Shenzhen (CUHKSZ) and the Shenzhen Rsearch
Institute of Big Data (SRIBD).

## Star History

<a href="https://star-history.com/#FreedomIntelligence/HuatuoGPT&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=FreedomIntelligence/HuatuoGPT&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=FreedomIntelligence/HuatuoGPT&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=FreedomIntelligence/HuatuoGPT&type=Date" />
  </picture>
</a>
