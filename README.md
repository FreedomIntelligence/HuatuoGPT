# HuatuoGPT(åä½—GPT): taming large language models to follow medical instruction.

# ğŸ‘¨â€âš•ï¸Introduction

Welcome to the repository of HuatuoGPT, a large language model (LLM) trained on a vast Chinese medical corpus. Our objective with HuatuoGPT is to construct a more professional â€˜ChatGPTâ€™ for medical consultation scenarios. Here is a list of what has been released:

1. Huatuo-200K (Medical Instruction Dataset): a  high-qulity instruction dataset that sampled from HuatuoGPT training corpus.
2. Medical-Chat LLMs: weights of a set of popular LLMs trained on our Huatuo-200K.
3. HuatuoGPT: a currently beta version HuatuoGPT online.
4. Medical evaluation benchmark: an evaluation method used to evaluate several LLMs in medical domain.

Hope our repository will help enhance the field of intelligent medicine and its related applications.

# ğŸ’­Motivation

- To address the growing demand for quick medical consultations both online and in hospitals that do not necessarily require deep medical knowledge. We believe that LLMs like HuatuoGPT can be effectively utilized to meet these demands, freeing up physiciansâ€™ time and energy for more complex cases.
- To provide open data for training medical LLMs. Building high-quality instruction training data for LLMs is essential, but it can be also challenging. We have constructed medical instruction data using various methods and made it publicly available. This dataset can be combined with other datasets to train one's own medical 'ChatGPT'.
- To emphasize the importance of carefully evaluating the ability of medical LLMs before using them to offer medical assistance to patients. We recognize the potential benefits of LLMs in the medical field, but also acknowledge the need for thorough evaluation and testing to ensure patient safety and accurate diagnoses.

# ğŸ“šData

We have gathered hundreds of billion tokens of online data to create a high-quality medical training corpus for HuatuoGPT. We also selected 200K pieces from this dataset as medical instruction data, named Huatuo-200K. This subset is adequate for fine-tuning LLMs to create ChatGPT-like consultant models. We are releasing this resource to the public, enabling anyone to train their very own intelligent doctor. 

| Category                          | Scale | Descriptions                                                 | Examples                                                     |
| --------------------------------- | ----- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Medical Instructions              | 30K   | Medical instructions generated with self-instruct. Seed instructions are manually written by us. | ğŸ’¬ï¼šç°åœ¨è¯¥åšä»€ä¹ˆå‡†å¤‡æ‰‹æœ¯å‰çš„èº«ä½“æ£€æŸ¥ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šåœ¨å‡†å¤‡æ‰‹æœ¯å‰çš„èº«ä½“æ£€æŸ¥ä¸­ï¼Œéœ€è¦è¿›è¡Œä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š<br>&emsp;&emsp;1. äº†è§£ç—…äººçš„ç—…å²ï¼šäº†è§£ç—…äººçš„ç—…å²ï¼ŒåŒ…æ‹¬æ—¢å¾€ç—…å²ã€æ‰‹æœ¯å²ã€è¯ç‰©è¿‡æ•å²ã€å®¶æ—ç—…å²ç­‰ï¼Œä»¥ä¾¿è¯„ä¼°æ‰‹æœ¯é£é™©å’Œåˆ¶å®šæ‰‹æœ¯æ–¹æ¡ˆã€‚<br>&emsp;&emsp;2. è¿›è¡Œä½“æ ¼æ£€æŸ¥ï¼šè¿›è¡Œå…¨é¢çš„ä½“æ ¼æ£€æŸ¥ï¼ŒåŒ…æ‹¬æµ‹é‡èº«é«˜ã€ä½“é‡ã€è¡€å‹ã€å¿ƒç‡ã€å‘¼å¸â€¦ |
| Medical Single-Round consultants  | 130K  | Medical single-round medical consultants in real-world scenarios, in which the doctor's response is more specific and can address the user's medical concerns. | ğŸ’¬ï¼šæˆ‘æœ‰ç³–å°¿ç—…è¡€å‹160-90.è¯¥åƒä»€ä¹ˆé™å‹è¯ï¼Ÿç©ºè…¹è¡€ç³–8å·¦å³ã€‚é¤å11å·¦å³ã€‚æˆ‘æœ‰ç³–å°¿ç—…è¡€å‹160-90.è¯¥åƒä»€ä¹ˆé™å‹è¯ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šé˜…è¯»ä½ çš„æé—®æè¿°ï¼Œç»¼åˆåˆ†æç—‡çŠ¶ï¼Œä½“å¾ï¼Œç³–å°¿ç—…ï¼Œè¡€å‹160-90.è¡€å‹ç•¥é«˜ï¼Œç©ºè…¹è¡€ç³–8å·¦å³ã€‚é¤å11å·¦å³ã€‚è¡€ç³–ç•¥é«˜ã€‚å£æœä¾é‚£æ™®åˆ©ï¼ŒäºŒç”²åŒèƒç‰‡æ²»ç–—ï¼Œæ–°è¿‘ç ”ç©¶ï¼ŒäºŒç”²åŒèƒç‰‡ï¼Œå¯ä»¥æŠ‘åˆ¶é£Ÿæ¬²ï¼Œä¿ƒè¿›è‚ é“è •åŠ¨ï¼Œé¢„é˜²ç™Œç—‡ï¼Œé¢„é˜²å¿ƒè„‘è¡€ç®¡ç—…ï¼Œå»¶é•¿å¯¿å‘½ä½œç”¨ã€‚ |
| Medical Multi-Round consultants   | 20K   | Medical multi-round conversations in real-world scenarios. Conversations include multiple rounds of questioning by doctors and continuous questioning by patients. | ğŸ’¬ï¼šé¢å¤´ä¸‹å·´ä¸‹é•¿æ»¡ç—˜ç—˜ã€‚å°±æ˜¯ä¸çŸ¥é“.<br>ğŸ‘¨â€âš•ï¸ï¼šä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå›°æ‰°å—ï¼Ÿæˆ‘å¯ä»¥ä¸ºä½ æä¾›å¸®åŠ©ã€‚è¯·é—®æ‚¨å¹³æ—¶çš„é¥®é£Ÿä¹ æƒ¯ã€ç¡çœ å’ŒæŠ¤è‚¤ä¹ æƒ¯æ€æ ·ï¼ŸåŒ–å¦†å“å’Œé˜²æ™’éœœçš„ä½¿ç”¨é¢‘ç‡å¦‚ä½•ï¼Ÿè¿˜æœ‰æ´é¢çš„é¢‘æ¬¡æ˜¯æ€æ ·çš„å‘¢ï¼Ÿ<br>ğŸ’¬ï¼šå¹³æ—¶æˆ‘åŸºæœ¬ä¸Šåªæ˜¯ç”¨æ°´ä¹³éœœï¼Œç„¶åæ´—é¢å¥¶åªç”¨ä¸€ç§ã€‚æ—¥å¸¸é˜²æ™’åªæœ‰åœ¨å¤å¤©æ—¶ä¼šä½¿ç”¨ï¼Œæˆ‘ä¸€èˆ¬ç”¨æ‚¦è¯—é£åŸçš„æ´—é¢å¥¶ä»¥åŠAHCçš„æŠ¤è‚¤äº§å“ï¼Œè¿™äº›éƒ½æ¯”è¾ƒæ¸©å’Œã€‚<br>ğŸ‘¨â€âš•ï¸ï¼šè°¢è°¢æ‚¨çš„æè¿°ã€‚é¦–å…ˆï¼Œå»ºè®®æ‚¨åœç”¨åŒ–å¦†å“å’Œæ™®é€šæŠ¤è‚¤å“ï¼Œä½¿ç”¨ç‰¹å®šçš„è¯å¦†å“ç‰Œï¼Œå¦‚é›…æ¼¾å’Œç†è‚¤æ³‰çš„æ´é¢æ³¡æ²«æˆ–æ´é¢ä¹³ï¼Œå¹¶é¿å…ä½¿ç”¨çƒ­æ°´æˆ–è‚¥çš‚æ¸…æ´—é¢éƒ¨ã€‚å»ºè®®æ‚¨å¤–ç”¨é˜¿è¾¾å¸•æ—å‡èƒ¶æ¶‚äºé¢éƒ¨ç—¤ç–®ï¼Œä¸€èˆ¬å¤œé—´ä½¿ç”¨ï¼Œæ¶‚æŠ¹åäº”åˆ†é’Ÿä¹‹åå¤–ç”¨åŒ»å­¦æŠ¤è‚¤å“æ¯”å¦‚é›…æ¼¾ã€è–‡èµ„ã€ç†è‚¤æ³‰æ¸…çˆ½å‹çš„èˆ’ç¼“é¢éœœï¼Œæˆ–è€…æ˜¯ç»´ç”Ÿç´ eä¹³è†â€¦. |
| General Instructions              | 10K   | Sampled general instructions. Mainly are generated with the method of self-instruct. | ğŸ’¬ï¼šç»™å‡ºä¸€ä¸ªæˆç†Ÿç”·äººæ½œç§»é»˜åŒ–é—´å¯ä»¥å»å®è·µçš„äººç”Ÿå“²å­¦ã€‚<br>ğŸ‘¨â€âš•ï¸ï¼šå›é¡¾ä½ æ›¾ç»æœ‰è¿‡çš„æ¯ä¸€æ¬¡ç»å†å¹¶ä»ä¸­å­¦ä¹ ï¼Œå¹¶è®¤çœŸè€Œæ·±åˆ»åœ°æ„Ÿå—è¿™äº›ç»å†å¸¦ç»™ä½ çš„æ•™è®­ã€‚è¿™æ ·ï¼Œä½ æ¯å¤©éƒ½å¯ä»¥ä»ä¸­è·å¾—æ›´å¤šè‡ªä¿¡å’ŒåŠ¨åŠ›ï¼Œæ›´å¥½åœ°å»é¢å¯¹ç”Ÿæ´»ä¸­çš„æ–°æŒ‘æˆ˜ã€‚ä»¥çœŸè¯šä¸ä»çˆ±ä¸ºåŸºç¡€ï¼Œä»¥å–„å¾…ä»–äººåŠè‡ªå·±ä¸ºåŸåˆ™ï¼Œä»¥åšçˆ±ä¸æœåŠ¡å…±å­˜ä¸ºå“²å­¦ï¼Œä»¥ä¸æ–­è‡ªæˆ‘è¶…è¶Šå’Œè¿›æ­¥ä¸ºç›®æ ‡â€¦. |
| General Multi-Round Conversations | 10K   | General conversations with ChatGPT. Mainly are from real conversations between users and ChatGPT. | ğŸ’¬ï¼šä¸‰çº§å…¬ç«‹åŒ»é™¢ç»©æ•ˆè€ƒæ ¸è‡ªè¯„æŠ¥å‘Šæ€ä¹ˆå†™<br>ğŸ‘¨â€âš•ï¸ï¼šç¼–å†™ä¸‰çº§å…¬ç«‹åŒ»é™¢ç»©æ•ˆè€ƒæ ¸è‡ªè¯„æŠ¥å‘Šéœ€è¦éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š<br>&emsp;&emsp;1.æ¢³ç†è€ƒæ ¸æŒ‡æ ‡ï¼šä»”ç»†é˜…è¯»è€ƒæ ¸æŒ‡æ ‡ï¼Œå°†å…¶è¿›è¡Œåˆ†ç±»ã€æ¢³ç†ï¼Œç†æ¸…è€ƒæ ¸æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»å’Œé‡è¦æ€§â€¦ <br>ğŸ’¬ï¼šè¯·å†™ä¸€ç¯‡6000å­—å·¦å³çš„ç¤ºä¾‹ã€‚<br>ğŸ‘¨â€âš•ï¸ï¼šå¾ˆæŠ±æ­‰ï¼Œæˆ‘æ˜¯ä¸€åäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹ï¼Œæ— æ³•ä¸ºæ‚¨æä¾›é•¿ç¯‡6000å­—å·¦å³çš„ç¤ºä¾‹ã€‚æˆ‘å¯ä»¥æä¾›ä¸€äº›å†™ä½œå»ºè®®å’ŒèŒƒä¾‹ï¼Œä»¥å¸®åŠ©æ‚¨ç¼–å†™ä¸‰çº§å…¬ç«‹åŒ»é™¢ç»©æ•ˆè€ƒæ ¸è‡ªè¯„æŠ¥å‘Šã€‚<br>&emsp;&emsp;1. ç®€ä»‹â€¦. |

# Models

- Bloomz-Huatuo200K
- Phoenix-Huatuo200K
- ChatGLM-Huatuo200K
- Llama-Huatuo200K


## Evaluations

- Huatuo200K
- ChatDoctor
- DoctorGLM

- Huatuo-Llama-Med-Chinese


# HuatuoGPT

See our model in [https://www.huatuogpt.cn/](https://www.huatuogpt.cn/) (API key required). Note that it is still in progressing.

# Limitations

Our goal with HuatuoGPT is to address the need for quick medical consultations, rather than replace doctors or provide full medical support to patients. However, our model does have several limitations that must be taken into consideration:

- Misunderstandings: As with all language models, there is a risk of misunderstandings or misinterpretations, especially when dealing with medical jargon or complex conditions. In this scenario, our models may give wrong answers.
- Hallucinations: Large language models can sometimes generate responses that do not make sense or are completely unrelated to the given input. These "hallucinations" can be especially problematic when users are not familiar with the concepts being discussed, as they may not be able to easily recognize the errors in the model's output. These "hallucinations" can be a challenge to detect and avoid.
- Bias: LLMs are trained on large datasets, which can inadvertently introduce bias into the model's responses. Additionally, care should be taken to ensure that the model is not used to perpetuate biases in medical treatment.

# Acknowledgement



# References

HuatuoGPT, taming Language models to follow medical instructions in conversation.

Less is more, taming Language models to follow medical instructions in conversation using 200k samples.

