# HuatuoGPT (åä½—GPT), Towards Taming Language Models To Be a Doctor.

## âœ¨ Latest News
- [05/25/2023]: Release the [tech report](https://arxiv.org/pdf/2305.15075.pdf) and the HuatuoGPT [demo](https://www.huatuogpt.cn/).

## âš¡ Introduction
Welcome to the repository of HuatuoGPT, a large language model (LLM) trained on a vast Chinese medical corpus. Our objective with HuatuoGPT is to construct a more professional â€˜ChatGPTâ€™ for medical consultation scenarios. 

Here is a list of what has been released:

1. HuatuoGPT-SFT-data: A hybrid SFT data capitalizing on both strengths to endow the model with Doctor-like and Patient-friendly characteristics.
2. HuatuoGPT model: HuatuoGPT model weights and the online demo.
3. Medical evaluation benchmark: an evaluation method used to evaluate LLMs in medical scenarios.

<div align=center>
<img src="assets/huatuo.png" width = "640" alt="HuatuoGPT" align=center/>
</div>


## ğŸ’­ Motivation
- To address the growing demand for quick medical consultations both online and in hospitals that do not necessarily require deep medical knowledge. We believe that LLMs like HuatuoGPT can be effectively utilized to meet these demands, freeing up physiciansâ€™ time and energy for more complex cases.
- To provide open data for training medical LLMs. Building high-quality instruction training data for LLMs is essential, but it can be also challenging. We have constructed medical instruction data using various methods and made it publicly available. This dataset can be combined with other datasets to train one's own medical 'ChatGPT'.
- To emphasize the importance of carefully evaluating the ability of medical LLMs before using them to offer medical assistance to patients. We recognize the potential benefits of LLMs in the medical field, but also acknowledge the need for thorough evaluation and testing to ensure patient safety and accurate diagnoses.

## ğŸ“š Data

### Overview

To leverage the best of both distilled data (from ChatGPT) and real-world data (from Doctors), we firstly fine-tune  `HuatuoGPT`  using four types of data:

| Type                          | Scale | Descriptions                                                 | Examples                                                     |
| ------------------------------------ | ----- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Distilled Instructions from ChatGPT  | 61,400   | Medical instructions generated with self-instruct. Seed instructions are manually written by us. | ğŸ’¬ï¼šç°åœ¨è¯¥åšä»€ä¹ˆå‡†å¤‡æ‰‹æœ¯å‰çš„èº«ä½“æ£€æŸ¥ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šåœ¨å‡†å¤‡æ‰‹æœ¯å‰çš„èº«ä½“æ£€æŸ¥ä¸­ï¼Œéœ€è¦è¿›è¡Œä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š<br>&emsp;&emsp;1. äº†è§£ç—…äººçš„ç—…å²ï¼šäº†è§£ç—…äººçš„ç—…å²ï¼ŒåŒ…æ‹¬æ—¢å¾€ç—…å²ã€æ‰‹æœ¯å²ã€è¯ç‰©è¿‡æ•å²ã€å®¶æ—ç—…å²ç­‰ï¼Œä»¥ä¾¿è¯„ä¼°æ‰‹æœ¯é£é™©å’Œåˆ¶å®šæ‰‹æœ¯æ–¹æ¡ˆã€‚<br>&emsp;&emsp;2. è¿›è¡Œä½“æ ¼æ£€æŸ¥ï¼šè¿›è¡Œå…¨é¢çš„ä½“æ ¼æ£€æŸ¥ï¼ŒåŒ…æ‹¬æµ‹é‡èº«é«˜ã€ä½“é‡ã€è¡€å‹ã€å¿ƒç‡ã€å‘¼å¸â€¦ |
| Real-world Instructions from Doctors | 69,768  | Medical single-round medical consultants in real-world scenarios, in which the doctor's response is more specific and can address the user's medical concerns. | ğŸ’¬ï¼šæˆ‘æœ‰ç³–å°¿ç—…è¡€å‹160-90.è¯¥åƒä»€ä¹ˆé™å‹è¯ï¼Ÿç©ºè…¹è¡€ç³–8å·¦å³ã€‚é¤å11å·¦å³ã€‚æˆ‘æœ‰ç³–å°¿ç—…è¡€å‹160-90.è¯¥åƒä»€ä¹ˆé™å‹è¯ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šé˜…è¯»ä½ çš„æé—®æè¿°ï¼Œç»¼åˆåˆ†æç—‡çŠ¶ï¼Œä½“å¾ï¼Œç³–å°¿ç—…ï¼Œè¡€å‹160-90.è¡€å‹ç•¥é«˜ï¼Œç©ºè…¹è¡€ç³–8å·¦å³ã€‚é¤å11å·¦å³ã€‚è¡€ç³–ç•¥é«˜ã€‚å£æœä¾é‚£æ™®åˆ©ï¼ŒäºŒç”²åŒèƒç‰‡æ²»ç–—ï¼Œæ–°è¿‘ç ”ç©¶ï¼ŒäºŒç”²åŒèƒç‰‡ï¼Œå¯ä»¥æŠ‘åˆ¶é£Ÿæ¬²ï¼Œä¿ƒè¿›è‚ é“è •åŠ¨ï¼Œé¢„é˜²ç™Œç—‡ï¼Œé¢„é˜²å¿ƒè„‘è¡€ç®¡ç—…ï¼Œå»¶é•¿å¯¿å‘½ä½œç”¨ã€‚ |
| Distilled Conversations from ChatGPT     | 68,888   | Medical multi-turn conversations are created using two ChatGPTs: one acting as the patient and the other as the doctor. The conversations are based on real-world patient cases and their corresponding treatment. | ğŸ’¬ï¼šUser: æˆ‘æƒ³é—®ä¸€ä¸‹å¤§å¤«ï¼Œæˆ‘å‘çƒ§ä¸‰å¤©åï¼Œç¬¬å››å¤©å…¨èº«èµ·äº†ç–¹å­ï¼Œæ˜¯æ€ä¹ˆå›äº‹å‘¢ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼š~æ‚¨å¥½ï¼æ„Ÿè°¢æ‚¨çš„æé—®ã€‚æ‚¨çš„ç—‡çŠ¶å¯èƒ½ä¸å¹¼å„¿æ€¥ç–¹ç›¸å…³ã€‚å¹¼å„¿æ€¥ç–¹æ˜¯ä¸€ç§å¸¸è§çš„å„¿ç«¥ç—…ï¼Œç—…å› ä¸ºäººç±»ç–±ç–¹ç—…æ¯’6å‹æˆ–7å‹æ„ŸæŸ“å¼•èµ·ã€‚å®ƒçš„ç—‡çŠ¶åŒ…æ‹¬ï¼š<br> 1. å‘çƒ­ n2. å…¨èº«èµ·çº¢è‰²ç–¹å­ <br> 3. å£è…”å†…åŠå’½å–‰éƒ¨æœ‰å°æ°´æ³¡<br> 4. ç—…ç¨‹ä¸€èˆ¬ä¸º3-5å¤©å·¦å³...<br>ğŸ’¬ï¼šè°¢è°¢å¤§å¤«ï¼é™¤äº†å¹¼å„¿æ€¥ç–¹è¿˜æœ‰å…¶ä»–å¯èƒ½å—ï¼Ÿæˆ‘ä¹‹å‰æœ‰æ„Ÿå†’ï¼Œè¿™å’Œç–¹å­æœ‰å…³ç³»å—ï¼Ÿ<br>ğŸ‘¨â€âš•ï¸ï¼šé™¤äº†å¹¼å„¿æ€¥ç–¹ï¼Œç—…æ¯’æ€§æ„Ÿå†’å’Œå…¶ä»–ç—…æ¯’æ„ŸæŸ“ä¹Ÿæœ‰å¯èƒ½å¼•èµ·ç–¹å­ï¼Œä½†è¿™ç§æƒ…å†µç›¸å¯¹è¾ƒå°‘è§ã€‚åœ¨æ‚¨çš„æƒ…å†µä¸‹ï¼Œæ„Ÿå†’å¯èƒ½æ˜¯ä¸€ç§è¯±å‘å› ç´ ï¼Œä½†ä¸»è¦è¿˜æ˜¯ç”±ç—…æ¯’æ„ŸæŸ“å¼•èµ·çš„ç–¹å­... |
| Real-world Conversations with Doctors      | 25,986   | Medical multi-round conversations in real-world scenarios. Conversations include multiple rounds of questioning by doctors and continuous questioning by patients. | ğŸ’¬ï¼šé¢å¤´ä¸‹å·´ä¸‹é•¿æ»¡ç—˜ç—˜ã€‚å°±æ˜¯ä¸çŸ¥é“.<br>ğŸ‘¨â€âš•ï¸ï¼šä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå›°æ‰°å—ï¼Ÿæˆ‘å¯ä»¥ä¸ºä½ æä¾›å¸®åŠ©ã€‚è¯·é—®æ‚¨å¹³æ—¶çš„é¥®é£Ÿä¹ æƒ¯ã€ç¡çœ å’ŒæŠ¤è‚¤ä¹ æƒ¯æ€æ ·ï¼ŸåŒ–å¦†å“å’Œé˜²æ™’éœœçš„ä½¿ç”¨é¢‘ç‡å¦‚ä½•ï¼Ÿè¿˜æœ‰æ´é¢çš„é¢‘æ¬¡æ˜¯æ€æ ·çš„å‘¢ï¼Ÿ<br>ğŸ’¬ï¼šå¹³æ—¶æˆ‘åŸºæœ¬ä¸Šåªæ˜¯ç”¨æ°´ä¹³éœœï¼Œç„¶åæ´—é¢å¥¶åªç”¨ä¸€ç§ã€‚æ—¥å¸¸é˜²æ™’åªæœ‰åœ¨å¤å¤©æ—¶ä¼šä½¿ç”¨ï¼Œæˆ‘ä¸€èˆ¬ç”¨æ‚¦è¯—é£åŸçš„æ´—é¢å¥¶ä»¥åŠAHCçš„æŠ¤è‚¤äº§å“ï¼Œè¿™äº›éƒ½æ¯”è¾ƒæ¸©å’Œã€‚<br>ğŸ‘¨â€âš•ï¸ï¼šè°¢è°¢æ‚¨çš„æè¿°ã€‚é¦–å…ˆï¼Œå»ºè®®æ‚¨åœç”¨åŒ–å¦†å“å’Œæ™®é€šæŠ¤è‚¤å“ï¼Œä½¿ç”¨ç‰¹å®šçš„è¯å¦†å“ç‰Œï¼Œå¦‚é›…æ¼¾å’Œç†è‚¤æ³‰çš„æ´é¢æ³¡æ²«æˆ–æ´é¢ä¹³ï¼Œå¹¶é¿å…ä½¿ç”¨çƒ­æ°´æˆ–è‚¥çš‚æ¸…æ´—é¢éƒ¨ã€‚å»ºè®®æ‚¨å¤–ç”¨é˜¿è¾¾å¸•æ—å‡èƒ¶æ¶‚äºé¢éƒ¨ç—¤ç–®ï¼Œä¸€èˆ¬å¤œé—´ä½¿ç”¨ï¼Œæ¶‚æŠ¹åäº”åˆ†é’Ÿä¹‹åå¤–ç”¨åŒ»å­¦æŠ¤è‚¤å“æ¯”å¦‚é›…æ¼¾ã€è–‡èµ„ã€ç†è‚¤æ³‰æ¸…çˆ½å‹çš„èˆ’ç¼“é¢éœœï¼Œæˆ–è€…æ˜¯ç»´ç”Ÿç´ eä¹³è†â€¦ |

### Download
- [HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1): The data used in the Supervised Fine-Tuning (SFT) stage of HuatuoGPT.

  

## ğŸ‘¨â€âš•ï¸ Model

### Model Access

- [HuatuoGPT-v1](https://huggingface.co/FreedomIntelligence/HuatuoGPT-7b-v1) (Currently unavailable, coming soon.)

### Deploy

Firstly, you should install all required packages
```bash
pip install -r requirements.txt
```

Please make sure you have download our model weights and run
```bash
python -m huatuo_cli_demo_stream.py --model-name $model_dir
```



## ğŸš€ Demo

Try our model in [https://www.huatuogpt.cn/](https://www.huatuogpt.cn/). Note that it is still in progressing.

<!-- ![demo_1](assets/demo_1.png) -->
<!-- ![demo_2](assets/demo_2.png) -->



## ğŸ§ Evaluations

### Automatic Evaluation Using GPT-4

<div align=center>
<img src="assets/eval1.png"  alt="eval1" align=center/>
</div>

### Expert  Evaluation

* Single-turn question

|  | Win | Loss |
| --------------------------- | :--: | :--: |
| **HuatuoGPT** vs. DoctorGLM         |  98%  |  2%  |
| **HuatuoGPT** vs. ChatGPT       |  **52%**  |  48%  |

* Multi-turn diagnosis

|  | Win | Loss |
| --------------------------- | :--: | :--: |
| **HuatuoGPT** vs. DoctorGLM         |  86%  |  14%  |
| **HuatuoGPT** vs. ChatGPT       |  **58%**  |  42%  |



## ğŸ¤– Limitations

Our goal with HuatuoGPT is to address the need for quick medical consultations, rather than replace doctors or provide full medical support to patients. However, our model does have several limitations that must be taken into consideration:

- Misunderstandings: As with all language models, there is a risk of misunderstandings or misinterpretations, especially when dealing with medical jargon or complex conditions. In this scenario, our models may give wrong answers.
- Hallucinations: Large language models can sometimes generate responses that do not make sense or are completely unrelated to the given input. These "hallucinations" can be especially problematic when users are not familiar with the concepts being discussed, as they may not be able to easily recognize the errors in the model's output. These "hallucinations" can be a challenge to detect and avoid.
- Bias: LLMs are trained on large datasets, which can inadvertently introduce bias into the model's responses. Additionally, care should be taken to ensure that the model is not used to perpetuate biases in medical treatment.

## Acknowledgement

We are aware that our works are inspired by the following works, including but not limited to

- Bloom: https://huggingface.co/bigscience/bloom
- Self-instruct: https://github.com/yizhongw/self-instruct

Without these, nothing could happen in this repository.

## Citation
```angular2
@article{huatuogpt-2023,
  title={HuatuoGPT, Towards Taming Language Models To Be a Doctor},
  author={Hongbo Zhang and Junying Chen and Feng Jiang and Fei Yu and Zhihong Chen and Jianquan Li and Guiming Chen and Xiangbo Wu and Zhiyi Zhang and Qingying Xiao and Xiang Wan and Benyou Wang and Haizhou Li},
  journal={arXiv preprint arXiv:2305.15075},
  year={2023}
}
```

We are from the School of Data Science, the Chinese University of Hong Kong, Shenzhen (CUHKSZ) and the Shenzhen Rsearch
Institute of Big Data (SRIBD).

## Star History

<a href="https://star-history.com/#FreedomIntelligence/HuatuoGPT&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=FreedomIntelligence/HuatuoGPT&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=FreedomIntelligence/HuatuoGPT&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=FreedomIntelligence/HuatuoGPT&type=Date" />
  </picture>
</a>
